{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An attempt at a more formal grammar\n",
    "\n",
    "```\n",
    "function_to_call  ::= <wordcharacters>\n",
    "parameters        ::= \"\" | \"(\" ( wordcharacters | number ) \")\"\n",
    "as_name           ::= \"\" | \"as\" <whitespace> <wordcharacters>\n",
    "column_name       ::= as_name | function_to_call\n",
    "reference         ::= \"\" |  \"[\" number \"]\"\n",
    "unique_mark       ::= \"\" | \"*\"\n",
    "column_definition ::= <function_to_call> <parameters> <whitespace> \\\n",
    "                      <as_name> <whitespace> <reference> <unique_mark>\n",
    "df_size           ::= \"\" |  \"[\" integer \"]\"\n",
    "df_sep            ::= \"--\" (\"-\"*)\n",
    "df_definition     ::= <wordcharacters> <df_size> <newline> <df_sep> <newline> \\\n",
    "                      (<column_definition>*) <newline> <newline>\n",
    "language_spec     ::= <def_definition>*\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import (register_line_magic, register_cell_magic,\n",
    "                                register_line_cell_magic)\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Config for generator\n",
    "# Please tweak these numbers sensibly, keeping in mind unique faker values and reference linking\n",
    "DEFAULT_DF_SIZE = 99   # default number of rows per DataFrame\n",
    "MAX_REPEATS = 4        # maximum number of times a reference can repeat (1 = repeated at most once)\n",
    "ORPHANED_UNIQUES = 0.2 # % of uniques references which won't be used \n",
    "IS_DEBUG = True        # debug flag for a few print statements.  not sure if and how __debug__ should be used instead?\n",
    "# Global variables\n",
    "reference_dict = {} # persistent reference for the reference columns, dict of sets\n",
    "fake = Faker()\n",
    "\n",
    "# Helper functions for casting parameters to int, float or string\n",
    "def cast_parameter(x):\n",
    "    if x is None:\n",
    "        return x\n",
    "    elif type(x) != str:\n",
    "        raise ValueError('Input must be a string or None')\n",
    "    try:\n",
    "        a = float(x)\n",
    "        b = int(a)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    else:\n",
    "        if a == b:\n",
    "            return b\n",
    "    try:\n",
    "        a = float(x)\n",
    "    except ValueError:\n",
    "        return x\n",
    "    else:\n",
    "        return a\n",
    "    \n",
    "def dprint(s):\n",
    "    if IS_DEBUG:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generate a singular value based on the Faker function and reference used\n",
    "# Uniqueness is checked against the set \"set_for_uniqueness\", a local reference for the caller is needed\n",
    "# Pass None to set_for_uniqueness for no uniqueness requirement\n",
    "\n",
    "def gen_data(_, function_name,parameter,set_for_uniqueness):\n",
    "    # QUESTION: not sure if passing by set() by reference for set_for_uniqueness will mess up by panda's\n",
    "    #           concurrency. It SEEMS OK, but have not tested rigorously\n",
    "    # TODO:     parameter currently only takes 1 variable, need to convert to support multiple?\n",
    "    # TODO:     For some function_name like first_name, we can directly get the Provider list of possible\n",
    "    #           values and get a subset from it rather than repeatedly generating them\n",
    "    #               from faker.providers.person.en import Provider\n",
    "    #               set(Provider.first_names)\n",
    "    func = getattr(fake, function_name)\n",
    "    parameter = cast_parameter(parameter)\n",
    "    while(True):\n",
    "        value = func() if parameter is None else func(parameter)\n",
    "        if type(set_for_uniqueness) is set:\n",
    "            if value not in set_for_uniqueness:\n",
    "                set_for_uniqueness.add(value)\n",
    "                return value\n",
    "        else:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates data specifically for reference columns since the data needs to be shared amongst DataFrames\n",
    "\n",
    "def get_reference_column_data(reference_key, function_name,parameter,is_unique, count):\n",
    "    # ASSUMPTION: assuming that function_name and parameter for each reference is configured the same way\n",
    "    # first generate unique set of data if it doesn't already exist\n",
    "    if reference_key not in reference_dict.keys():\n",
    "        # FIXME: need to generate unique reference key first.\n",
    "        new_data = set()\n",
    "        while(len(new_data)<count):\n",
    "            new_data.add(gen_data(None, function_name,parameter,new_data))\n",
    "        reference_dict[reference_key] = new_data\n",
    "\n",
    "    if is_unique:\n",
    "        # if we are just looking for the unique data, we can returned the shuffled set\n",
    "        return random.sample(reference_dict[reference_key], len(reference_dict[reference_key]))\n",
    "    else:\n",
    "        # sample items to exclude some reference values\n",
    "        values = random.sample(reference_dict[reference_key], int(count*(1-ORPHANED_UNIQUES)))\n",
    "        # return sampled items (so they appear at least once) and fill the rest with the same sampled items\n",
    "        # based on the MAX_REPEATS configuration\n",
    "        return values + random.sample(values * (MAX_REPEATS-1),count - len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def fakedata(line, cell):\n",
    "\n",
    "    # We can probably do some checking here, such as:\n",
    "    # * no dataframes with same name\n",
    "    # * no duplicate names for columns with same name\n",
    "    \n",
    "    # Step 1, we parse the input into 2 separate dataframes\n",
    "    # dataframe_size contains the size for each dataframe\n",
    "    # dataframe_col_def contains column definitions of each dataframe\n",
    "    dataframe_size = pd.DataFrame(columns=['df_name', 'size'])\n",
    "    dataframe_size.set_index('df_name', inplace=True)\n",
    "    dataframe_col_def = pd.DataFrame(columns=['df_name', 'col_name', 'function', 'parameter', 'as_name', 'reference', 'unique_mark'])\n",
    "    dataframe_col_def.set_index(['df_name','col_name'], inplace=True)\n",
    "\n",
    "    # we split the dataframes out to process them one at a time\n",
    "    regex = re.compile(r'(?P<df_name>[\\w]+)(?: \\[(?P<df_size>\\d+)\\])?(?:\\n-+\\n)(?P<df_def>(?:.+\\n?)+)')\n",
    "    \n",
    "    for dfdict in [m.groupdict() for m in regex.finditer(cell.strip())]:\n",
    "        df_name = dfdict['df_name']\n",
    "        df_size = int(dfdict['df_size']) if dfdict['df_size'] is not None else DEFAULT_DF_SIZE\n",
    "        definitions = dfdict['df_def'].strip().split('\\n')\n",
    "\n",
    "        # we can check that there are no duplicated dataframe names\n",
    "        assert df_name not in dataframe_size.index, \"DataFrames with duplicated names found: {}\".format(df_name)\n",
    "        dataframe_size.loc[df_name] = [df_size]\n",
    "        \n",
    "        # break down each column of the DataFrame based on provided definitions\n",
    "        for d in definitions:\n",
    "            dpattern = r\"^(?P<function>\\w+)(?:(?:\\((?P<parameter>[-+]?\\d*\\.?\\d+|\\w+)\\))?)(?: as (?P<as_name>\\w*))?(?: \\[(?P<reference>\\d+)\\])?(?P<unique_mark>\\*)?$\"\n",
    "            dmatch = re.search(dpattern,d).groupdict()\n",
    "            col_name = dmatch['as_name'] if dmatch['as_name'] is not None else dmatch['function']\n",
    "            dmatch['unique_mark'] = dmatch['unique_mark'] is not None\n",
    "        \n",
    "            # we can check that there are no duplicated dataframe names\n",
    "            assert (df_name, col_name) not in dataframe_col_def.index, \"Columns with duplicated names found in DataFrame \\\"{}\\\": {}\".format(df_name, col_name)\n",
    "            dataframe_col_def.loc[(df_name, col_name),dataframe_col_def.columns] = [dmatch['function'], dmatch['parameter'], dmatch['as_name'], dmatch['reference'], dmatch['unique_mark']]\n",
    "    \n",
    "    # we don't need as_name anymore since column names are set, so let's drop it\n",
    "    dataframe_col_def.drop(columns=['as_name'], inplace=True)\n",
    "\n",
    "    # These are the parsed input\n",
    "    dprint(dataframe_size)\n",
    "    dprint(dataframe_col_def)\n",
    "    \n",
    "    # Now let's create the dataframes!\n",
    "    \n",
    "    # Step 2, pre-generate all references first\n",
    "    # for each reference that has  a unique_mark, we generate the exact amount needed to fill its dataframe\n",
    "    # if no unique, just find the highest\n",
    "    ref_df = dataframe_col_def.join(dataframe_size, how='inner', on='df_name')\n",
    "    ref_df = ref_df.groupby(['reference','function','parameter','unique_mark'], as_index=False)[['size']].max()\n",
    "    ref_check = ref_df[ref_df.reset_index().duplicated(subset=['reference','unique_mark'])]\n",
    "    ref_df.sort_values(by=['unique_mark'], ascending=False, inplace=True)\n",
    "    ref_df.drop_duplicates(subset=['reference','function','parameter'], inplace=True)\n",
    "    dprint(ref_df)\n",
    "    assert len(ref_check) == 0, 'Inconsistent \"function\" and \"parameter\" definitions for references: {}'.format(ref_check.reference.tolist())\n",
    "    \n",
    "    while(False):\n",
    "        # create DataFrame \n",
    "        df = pd.DataFrame(index=range(0,df_size), columns=column_info.keys())\n",
    "        \n",
    "        #populate each column with data\n",
    "        for k,v in column_info.items():\n",
    "            if v['reference'] is not None:\n",
    "                # this is a reference column, we need to reference our references of the reference\n",
    "                # since we're demonstrating One to Many\n",
    "                df[k] = get_reference_column_data(v['reference'],v['function'],v['parameter'],v['unique_mark'], df_size)\n",
    "            else:\n",
    "                df[k] = df[k].apply(gen_data, args=(v['function'],v['parameter'],set() if v['unique_mark'] else None))\n",
    "        \n",
    "        # assign the created DataFrame as a global variable using the provided name\n",
    "        globals()[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%fakedata\n",
    "persons [20]\n",
    "-------\n",
    "first_name\n",
    "last_name*\n",
    "phone_number\n",
    "random_number(5) as customer_number [1]*\n",
    "\n",
    "purchasesA [20]\n",
    "---------\n",
    "isbn10\n",
    "credit_card_full\n",
    "random_number(3) as price\n",
    "random_number(5) as customer_number [1]\n",
    "\n",
    "purchasesB [40]\n",
    "---------\n",
    "isbn10\n",
    "credit_card_full\n",
    "random_number(3) as price\n",
    "random_number(5) as customer_number [1]\n",
    "\n",
    "purchasesC [30]\n",
    "---------\n",
    "isbn10\n",
    "credit_card_full\n",
    "random_number(3) as price\n",
    "random_number(5) as customer_number [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for purchases['customer_number'] reference values\n",
    "# how many times distribution of number of times each customer_number appeared\n",
    "purchases['customer_number'].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing my cast_parameter() helper function\n",
    "# l = [\"-1\",\"12313\",\"-1.0.0\",\"1.546\",\"-.4\",\"+31\",\"+.7\",\"abc123\",\"d7f8g8h8jh\",\"1e2\",\"-6e-3\",\"\", None]\n",
    "# for v in l:\n",
    "#     a = cast_parameter(v)\n",
    "#     print(\"{} value: [{}]\".format(type(a),a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
