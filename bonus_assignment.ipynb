{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Realistic Example Data\n",
    "This __optional bonus assignment__ is worth up to 5% of your final grade. It must be handed in by directly mailing the instructor (Christopher Brooks, brooksch@umich.edu) the assignment no later than Sunday September 27th at 11:59pm EST. This is not an all-or-nothing assignment, partial grades will be provided as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Overview\n",
    "A constant need I have when teaching pandas is finding compelling example data to work from. Good example data is real-world, messy enough to need some manipulation, and fits reasonable constraints for a given problem. For instance, if I want to demonstrate joining multiple `DataFrame` together I might want one which is about people and one which is about purchases, where every person has an identifier and a bunch of personal information, and every purchase is linked to a given person. This is much more compelling then a bunch of random `np.ndarray` lists that I create inline while trying to give a lecture!\n",
    "\n",
    "In addition, I'm taken by domain specific languages, and in this part of the assignment you are required to build a processor for a simple domain specific language I have invented for the purpose of describing pandas `DataFrame` structures! It is expected that you will demonstrate your knowledge of regex here in particular.\n",
    "\n",
    "Here's an example of the language I've created for this part of the assignment:\n",
    "```\n",
    "persons\n",
    "-------\n",
    "first_name\n",
    "last_name*\n",
    "phone_number\n",
    "random_number(5) as customer_number [1]*\n",
    "\n",
    "purchases\n",
    "---------\n",
    "isbn10\n",
    "credit_card_full\n",
    "random_number(3) as price\n",
    "random_number(5) as customer_number [1]\n",
    "```\n",
    "\n",
    "In this example I describe two `DataFrame` objects by underlying a string with two or more hyphens. The string (`persons` and `purchases`) should be used as the variable name for the `DataFrame` objects created, and the language will always separate multiple `DataFrame` definitions by whitespace and hyphens as shown. Each column in the `DataFrame` is described on its own line with a string (e.g. `first_name*`). The string defines the column as follows:\n",
    "\n",
    "1. The first word (e.g. `first_name` or `random_number(3)`) describes a function  and optional set of parameters to be called against a common `faker` object (an instance of the `Faker` class) for each entry in the resulting `DataFrame`. For instance, a value of `isbn10` implies that `faker.isbn10()` be called (note the default parameters), while a value of `random_number(5)` implies that `faker.random_number(5)` be called (with my supplied parameters).\n",
    "2. The first word *may* be followed by some whitespace and then an `as` statement. The `as` statement denotes that the following word be used as the name of the column. For instance, `random_number(3) as price` means that I'm looking for a column named `price` where every instance in the column is of a `faker.random_number(3)` invocation.\n",
    "3. If there is no `as` statement the name of the column should be the name of the function (with no parameters) I supplied, e.g. `first_name`.\n",
    "4. The definition may include a reference in the format `[#]` where the # sign is any number. This reference will be used across tables to show that the data in those tables should be of a similar set of values. This is so I can join between tables, which is a common need to demonstrate. In the example given I want each of the tables to have a column called `customer_number` where the data values in the column are such that `set(column1)==set(column2)`. Note that this doesn't mean the columns should be the same (they shouldn't), but just that they should include only 100% overlapping data. See point 6 for data distribution. **Clarification: In the example here, all of the `customer_number` values in `persons` should be unique (hence the `*`), but in the referencing collumn `purchases[customer_number]` the uniqueness is relaxed. This means that there does not have to be a 1:1 mapping as implied, and that it is a 1:many mapping, and in this case since `purchases` has repeated data you could verify this with `set(purchases['customer_number']).issubset(set(persons['customer_number']))`.**\n",
    "5. The sentence may end with a `*`. This indicates that the column described should be made up of unique data (no repeated elements). For instance, you wouldn't want a customer to be accidently assigned the customer number of another person! In the example above I decided I wanted the `last_name` of the persons and their `customer_number` to be unique in that table.\n",
    "6. By default a column should have 25% repeated data. e.g. something close to `len(column)==len(set(column))*1.25`. This lets me demonstrate operations such as left joins easily.\n",
    "7. By default, the length of each `DataFrame` created should be 99 items. This is both reasonable for most demonstrations, and a homage to The Great One.\n",
    "8. The functionality described should be executed in a cell magic function called `%%fakedata`, where the remainder of the cell is the definition in plain text. See https://ipython.readthedocs.io/en/stable/config/custommagics.html for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An attempt at a more formal grammar\n",
    "\n",
    "```\n",
    "function_to_call  ::= <wordcharacters>\n",
    "parameters        ::= \"\" | \"(\" ( wordcharacters | number ) \")\"\n",
    "as_name           ::= \"\" | \"as\" <whitespace> <wordcharacters>\n",
    "column_name       ::= as_name | function_to_call\n",
    "reference         ::= \"\" |  \"[\" number \"]\"\n",
    "unique_mark       ::= \"\" | \"*\"\n",
    "column_definition ::= <function_to_call> <parameters> <whitespace> \\\n",
    "                      <as_name> <whitespace> <reference> <unique_mark>\n",
    "df_sep            ::= \"--\" (\"-\"*)\n",
    "df_definition     ::= <wordcharacters> <newline> <df_sep> <newline> \\\n",
    "                      (<column_definition>*) <newline> <newline>\n",
    "language_spec     ::= <def_definition>*\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: What is this `Faker` class?\n",
    "The `Faker` class defines a number of great functions that generate realistic data. The way it works is that you create a new instance of `Faker` with no parameters, then call various methods on that object which are predefined at https://faker.readthedocs.io/en/stable/\n",
    "\n",
    "This is demonstrated below showing a single entry into a `DataFrame` using the description above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import (register_line_magic, register_cell_magic,\n",
    "                                register_line_cell_magic)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Config for generator\n",
    "# Please tweak these numbers sensibly, keeping in mind unique faker values and reference linking\n",
    "DEFAULT_DF_SIZE = 99     # default number of rows per DataFrame\n",
    "ORPHANED_UNIQUES = 0.2   # % of uniques references which won't be used in non-unique references\n",
    "REPEAT_WEIGHTS = 1000    # Weight of chance for repeated values.  1 would be equal weights\n",
    "# I haven't really done the detailed math, on the higher end or larger DataFrame size, so maybe bigger variance end up\n",
    "# wouldn't be so big afterall.\n",
    "IS_DEBUG = True          # debug flag for a few print statements.  not sure if and how __debug__ should be used instead?\n",
    "\n",
    "assert (REPEAT_WEIGHTS >= 1) and type(REPEAT_WEIGHTS) == int, \"REPEAT_WEIGHTS must be type int and >= 1\"\n",
    "\n",
    "# Global variables\n",
    "fake = Faker()\n",
    "\n",
    "# This unique mapping helps maps a faker function to a provider's ordered dict\n",
    "# This is used for efficiently generating large amount of unique data if the data is taken from a dict\n",
    "# For example, fake.first_name() can possibly map to faker.providers.person.en_US.Provider.first_names\n",
    "unique_mapping = {\n",
    "    # There are probably more, but I'll stick to first_name() and last_name() for now\n",
    "    'first_name':'first_names',\n",
    "    'last_name':'last_names'\n",
    "}\n",
    "\n",
    "# Helper functions for casting parameters to int, float or string\n",
    "def cast_parameter(x):\n",
    "    if x is None:\n",
    "        return x\n",
    "    elif type(x) != str:\n",
    "        raise ValueError('Input must be a string or None')\n",
    "    try:\n",
    "        a = float(x)\n",
    "        b = int(a)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    else:\n",
    "        if a == b:\n",
    "            return b\n",
    "    try:\n",
    "        a = float(x)\n",
    "    except ValueError:\n",
    "        return x\n",
    "    else:\n",
    "        return a\n",
    "    \n",
    "def dprint(s):\n",
    "    if IS_DEBUG:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generate a singular value based on the Faker function and reference used\n",
    "# Uniqueness is checked against the set \"set_for_uniqueness\", a local reference for the caller is needed\n",
    "# Pass None to set_for_uniqueness for no uniqueness requirement\n",
    "\n",
    "def gen_data(_, function_name,parameter,set_for_uniqueness):\n",
    "    # QUESTION: not sure if passing by set() by reference for set_for_uniqueness will mess up by panda's\n",
    "    #           concurrency. It SEEMS OK, but have not tested rigorously\n",
    "    # TODO:     parameter currently only takes 1 variable, need to convert to support multiple?\n",
    "    # TODO:     For some function_name like first_name, we can directly get the Provider list of possible\n",
    "    #           values and get a subset from it rather than repeatedly generating them\n",
    "    #               from faker.providers.person.en import Provider\n",
    "    #               set(Provider.first_names)\n",
    "    func = getattr(fake, function_name)\n",
    "    parameter = cast_parameter(parameter)\n",
    "    while(True):\n",
    "        value = func() if parameter is None else func(parameter)\n",
    "        if type(set_for_uniqueness) is set:\n",
    "            if value not in set_for_uniqueness:\n",
    "                set_for_uniqueness.add(value)\n",
    "                return value\n",
    "        else:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generate a list of unique values from for supported functions listed in unique_mapping\n",
    "# The data is taken directly from the faker providers.\n",
    "\n",
    "def gen_unique_data(function_name, count):\n",
    "\n",
    "    assert function_name in unique_mapping.keys(), \"Unsupported function for gen_unique_data(): {}\".format(function_name)\n",
    "    \n",
    "    for p in fake.get_providers():\n",
    "        if function_name in dir(p) and unique_mapping[function_name] in dir(p):\n",
    "            assert len(getattr(p,\"last_names\").keys()) >= count, \"Insufficient data ({} available) from provider to generate {} \\\"{}\\\".\".format(len(getattr(p,\"last_names\").keys()), count, function_name)\n",
    "            return random.sample(getattr(p,\"last_names\").keys(), count)\n",
    "    # explicitly putting this here to remind myself it maybe possible that providers don't have the available data\n",
    "    # caller should ensure that data is provided and not None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates data specifically for reference columns since the data needs to be shared amongst DataFrames\n",
    "\n",
    "def get_reference_column_data(reference_key, function_name,parameter,is_unique, count, reference_dict):\n",
    "    if reference_key not in reference_dict.keys():\n",
    "        new_data = set()\n",
    "\n",
    "        if function_name in unique_mapping.keys():\n",
    "            unique_data = gen_unique_data(function_name, count)\n",
    "            if unique_data is not None:\n",
    "                new_data = unique_data\n",
    "        \n",
    "        while(len(new_data)<count):\n",
    "            new_data.add(gen_data(None, function_name,parameter,new_data))\n",
    "        reference_dict[reference_key] = new_data\n",
    "\n",
    "    if is_unique:\n",
    "        # if we are just looking for the unique data, we can returned the shuffled set\n",
    "        return random.sample(reference_dict[reference_key], count)\n",
    "    else:\n",
    "        # sample items to exclude some reference values\n",
    "\n",
    "        unique_count = int(len(reference_dict[reference_key])*(1-ORPHANED_UNIQUES))\n",
    "        picked_uniques = random.sample(reference_dict[reference_key], min(unique_count, count))\n",
    "        \n",
    "        # apart from initial copy how many additional copies needed?\n",
    "        w = [random.uniform(1,REPEAT_WEIGHTS) for _ in range(unique_count)]\n",
    "        random_picked = np.random.choice(picked_uniques, count - len(picked_uniques), p = [float(i)/sum(w) for i in w])\n",
    "        d = picked_uniques + list(random_picked)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def fakedata(line,cell):\n",
    "    reference_dict = {}\n",
    "    \n",
    "    # Step 1, we parse the input into 2 separate dataframes\n",
    "    # dataframe_size contains the size for each dataframe\n",
    "    # dataframe_col_def contains column definitions of each dataframe\n",
    "    dataframe_size = pd.DataFrame(columns=['df_name', 'size'])\n",
    "    dataframe_size.set_index('df_name', inplace=True)\n",
    "    dataframe_col_def = pd.DataFrame(columns=['df_name', 'col_name', 'function', 'parameter', 'as_name', 'reference', 'unique_mark'])\n",
    "    dataframe_col_def.set_index(['df_name','col_name'], inplace=True)\n",
    "\n",
    "    # we split the dataframes out to process them one at a time\n",
    "    regex = re.compile(r'(?P<df_name>[\\w]+)(?: \\[(?P<df_size>\\d+)\\])?(?:\\n-+\\n)(?P<df_def>(?:.+\\n?)+)')\n",
    "    for dfdict in [m.groupdict() for m in regex.finditer(cell.strip())]:\n",
    "        df_name = dfdict['df_name']\n",
    "        df_size = int(dfdict['df_size']) if dfdict['df_size'] is not None else DEFAULT_DF_SIZE\n",
    "        definitions = dfdict['df_def'].strip().split('\\n')\n",
    "\n",
    "        # we can check that there are no duplicated dataframe names\n",
    "        assert df_name not in dataframe_size.index, \"DataFrames with duplicated names found: {}\".format(df_name)\n",
    "        dataframe_size.loc[df_name] = [df_size]\n",
    "        \n",
    "        # break down each column of the DataFrame based on provided definitions\n",
    "        for d in definitions:\n",
    "            dpattern = r\"^(?P<function>\\w+)(?:(?:\\((?P<parameter>[-+]?\\d*\\.?\\d+|\\w+)\\))?)(?: as (?P<as_name>\\w*))?(?: \\[(?P<reference>\\d+)\\])?(?P<unique_mark>\\*)?$\"\n",
    "            dmatch = re.search(dpattern,d).groupdict()\n",
    "            col_name = dmatch['as_name'] if dmatch['as_name'] is not None else dmatch['function']\n",
    "            dmatch['unique_mark'] = dmatch['unique_mark'] is not None\n",
    "        \n",
    "            # we can check that there are no duplicated dataframe names\n",
    "            assert (df_name, col_name) not in dataframe_col_def.index, \"Columns with duplicated names found in DataFrame \\\"{}\\\": {}\".format(df_name, col_name)\n",
    "            dataframe_col_def.loc[(df_name, col_name),dataframe_col_def.columns] = [dmatch['function'], dmatch['parameter'], dmatch['as_name'], dmatch['reference'], dmatch['unique_mark']]\n",
    "    \n",
    "    # we don't need as_name anymore since we now have \"col_name\", so let's drop it\n",
    "    dataframe_col_def.drop(columns=['as_name'], inplace=True)\n",
    "\n",
    "    # These are the parsed input\n",
    "    #dprint(dataframe_size)\n",
    "    #dprint(dataframe_col_def)\n",
    "    \n",
    "    # Step 2, we create the dataframes with no data\n",
    "    for d in dataframe_size.itertuples():\n",
    "        df_name = d.Index\n",
    "        df_size = d.size\n",
    "        dfr = dataframe_col_def.loc[df_name].reset_index()\n",
    "        # create DataFrame one by one\n",
    "        df = pd.DataFrame(index=range(0,df_size), columns=dfr['col_name'].tolist())\n",
    "        # assign the Dataframe to the global space\n",
    "        globals()[df_name] = df\n",
    "\n",
    "    # Step 3, pre-generate all references first\n",
    "    # for each reference that has  a unique_mark, we generate the exact amount needed to fill its dataframe\n",
    "    # if no unique, just find the highest\n",
    "    ref_df = dataframe_col_def.join(dataframe_size, how='inner', on='df_name')\n",
    "    ref_df = ref_df.groupby(['reference','function','parameter','unique_mark'], as_index=False)[['size']].max()\n",
    "    ref_check = ref_df[ref_df.reset_index().duplicated(subset=['reference','unique_mark'])]\n",
    "    ref_df.sort_values(by=['unique_mark'], ascending=False, inplace=True)\n",
    "    ref_df.drop_duplicates(subset=['reference','function','parameter'], inplace=True)\n",
    "    #dprint(ref_df)\n",
    "    assert len(ref_check) == 0, 'Inconsistent \"function\" and \"parameter\" definitions for references: {}'.format(ref_check.reference.tolist())\n",
    "    for r in ref_df.itertuples():\n",
    "        # Assuming we're generating uniques here so random generation isn't needed.\n",
    "        get_reference_column_data(r.reference,r.function,r.parameter,r.unique_mark, r.size, reference_dict)\n",
    "    \n",
    "    # Step 4, populate all the data\n",
    "    for c in dataframe_col_def.join(dataframe_size, how='inner', on='df_name').itertuples():\n",
    "        df_name, col_name = c.Index\n",
    "        df_size = c.size\n",
    "        if c.reference is not None:\n",
    "            # this is a reference column, we need to reference our references of the reference\n",
    "            # since we're demonstrating One to Many\n",
    "            globals()[df_name][col_name] = get_reference_column_data(c.reference,c.function,c.parameter,c.unique_mark, df_size, reference_dict)\n",
    "        else:\n",
    "            if c.function in unique_mapping.keys():\n",
    "                unique_data = gen_unique_data(c.function, df_size)\n",
    "                if unique_data is not None:\n",
    "                    globals()[df_name][col_name] = unique_data\n",
    "                    continue\n",
    "            globals()[df_name][col_name] = globals()[df_name][col_name].apply(gen_data, args=(c.function,c.parameter,set() if c.unique_mark else None))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
